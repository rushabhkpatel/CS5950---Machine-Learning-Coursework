{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the all the basic packages and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def one_nn_index(test_sample, train_data):\n",
    "    \n",
    "    # test_sample the sample for which we need to predict the label\n",
    "    # train_data is the sample set. \n",
    "    # Note : train_data should be or shape (n,m) where n != 1 or else it causes issues, m is number of features\n",
    "    \n",
    "    min_dist, min_index = math.inf, 0\n",
    "    num_of_samples = train_data.shape[0]\n",
    "    for i in range(num_of_samples) :        \n",
    "        cal = math.sqrt(sum((train_data[i] - test_sample) ** 2))\n",
    "        if min_dist >= cal :\n",
    "            min_dist = cal\n",
    "            min_index = i\n",
    "    \n",
    "    return min_index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above function returns the index of the samples closes to test_sample. Here we use euclidean distance to find which is the nearest sample to the test_sample. We loop through the array and calculate the distance while simultaneously keep track of the minimum value and the minimum index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(sample, data, label):\n",
    "    ind = one_nn_index(sample, data)\n",
    "    return label[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above function acts as a driver (similar to knn.predict) to one_knn_index. It calls the one_knn_index function, gets the index and maps it to the corresponding label value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 412)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the iris dataset into default 3:1 split. We will now make predictions for the entire X_test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in X_test :\n",
    "    y_pred.append(knn(i, X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are stored in y_pred and we have the true labels in y_test. We can calculate the accuracy by using in np.mean function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the accuracy of the one nearest neighbour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the implementation of the conformal predictor. For each postulated label for every test sample pvalue is calculated and the value is returned. The conformity measure used is distance to nearest sample of different class divided by distance to nearest sample of same class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : We first update the postulated label since the test_sample is part of the sample set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 : We use nested for loops to get the conformity scores of the entire sample set. The outer for loop loops through the sample set and the inner for loop calculates the 2 distances (different class and same class). The calculated conformity score is stored in the 'conformity_scores' array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 : We  calculate the rank for  our postulated label and further calculate the p value. Here we divide rank by number of samples because n+1 in our case is equal to number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def calculate_pvalue(index, postulated_label, data, labels):\n",
    "    \n",
    "    # index is the index of the test sample in the entire sample\n",
    "    # postulated label is the assumed label for the test sample\n",
    "    # data is the sample set\n",
    "    # label is the actual labels of the sample set\n",
    "    \n",
    "    updated_label = labels\n",
    "    updated_label[index] = postulated_label # making the label for test sample to the postulated label\n",
    "    \n",
    "    \n",
    "    num_of_samples = data.shape[0]\n",
    "    conformity_scores = np.empty(num_of_samples)\n",
    "    \n",
    "    for i in range(num_of_samples) :\n",
    "        \n",
    "        min_dist_same, min_dist_diff = math.inf, math.inf #nearest distance to same class and difference class\n",
    "        \n",
    "        for j in range(num_of_samples) :\n",
    "            \n",
    "            if i != j : # Do not include the same sample in calculating the distances\n",
    "                # calculates the nearest distance to same and different class\n",
    "                cal = math.sqrt(sum((data[i] - data[j]) ** 2))\n",
    "                if  updated_label[i]  == updated_label[j] and min_dist_same > cal:\n",
    "                    min_dist_same = cal\n",
    "                elif updated_label[i]  != updated_label[j] and min_dist_diff > cal :\n",
    "                    min_dist_diff = cal\n",
    "                    \n",
    "        #Taking care of division problems\n",
    "        if min_dist_diff == 0 and min_dist_same == 0: \n",
    "            conformity_scores[i] = 0\n",
    "        elif min_dist_same == 0:\n",
    "            conformity_scores[i] = math.inf\n",
    "        elif min_dist_diff == 0:\n",
    "            conformity_scores[i] = 0 \n",
    "        else:\n",
    "            conformity_scores[i] = min_dist_diff / min_dist_same     \n",
    "        \n",
    "    # end of both the loops, at this point we have the coformity scores of all the samples\n",
    "    # we can calculate the rank of the test sample (we have it's index) followed by the p value\n",
    "    \n",
    "    rank = sum(conformity_scores[index] >= conformity_scores)\n",
    "    p = rank / num_of_samples # test sample is included in all samples, hence n+1 in our case is equal to num_of_samples \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8947368421052632"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_pvalue(15,0,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02631578947368421"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_pvalue(15, 1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02631578947368421"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_pvalue(15, 2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we calculate the average false p value. We calculate the p values for all samples but conditionally exclude true p values in the if condition. We store all the false p values in the array and then compute the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the average false p value\n",
    "\n",
    "false_pvalues = []\n",
    "\n",
    "size = X_test.shape[0]\n",
    "\n",
    "all_labels = list(set(y_test))\n",
    "for i in range(size) :\n",
    "    for j in range(len(all_labels)):\n",
    "        if y_test[i] != y_test[j]:\n",
    "            false_pvalues.append(calculate_pvalue(i, y_test[j], X_test, y_test))\n",
    "    \n",
    "average_pvalue = sum(false_pvalues) / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32340720221606645"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IONOSPHERE DATASET\n",
    "\n",
    "\n",
    "Below we run the code on the ionoshphere dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt(\"ionosphere.txt\",delimiter=\",\",usecols=np.arange(34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\", usecols=34, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in X_test :\n",
    "    y_pred.append(knn(i, X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ionosphere we can an accuray of 90% as shown above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pvalues = []\n",
    "\n",
    "size = X_test.shape[0]\n",
    "\n",
    "all_labels = list(set(y_test))\n",
    "for i in range(size) :\n",
    "    for j in range(len(all_labels)):\n",
    "        if y_test[i] != y_test[j]:\n",
    "            false_pvalues.append(calculate_pvalue(i, y_test[j], X_test, y_test))\n",
    "    \n",
    "average_pvalue = sum(false_pvalues) / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2920971074380166"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
